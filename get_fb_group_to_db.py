# Retrieve a group's posts, comments and likes and store them in an sqlite database (excluding the data from any user that disabled the sharing of their data with Facebook Graph API)
import urllib2
import json
import datetime
import csv
import time
import sqlite3

app_id = "xxxxxxxxxxxxxxxxxxxx" # app_id and app_secret can be generated by creating a new empty Facebook app
app_secret = "xxxxxxxxxxxxxxxxxxx" # DO NOT SHARE WITH ANYONE!

access_token = app_id + "|" + app_secret
group_id = '120245511500574' # New York Buy-Sell Group

ts = time.time()
st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d_%H:%M:%S')

conn = sqlite3.connect(group_id + '_facebook_group_' + st + '.sqlite3')
conn.text_factory = str
cur = conn.cursor()
cur.executescript("""
    CREATE TABLE likes (
        object_id TEXT,
        object_from_name TEXT,
        object_from_id TEXT,
        like_from_name TEXT,
        like_from_id TEXT
    );
    CREATE TABLE posts (
        post_id TEXT,
        post_from_name TEXT,
        post_from_id TEXT,
        post_message TEXT,
        post_created_time TEXT,
        post_updated_time TEXT,
        post_story TEXT,
        post_placename TEXT,
        post_name TEXT,
        post_type TEXT,
        post_link TEXT,
        post_num_likes TEXT,
        post_num_comments TEXT,
        post_num_shares TEXT
    );
    CREATE TABLE comments (
        post_id TEXT,
        comment_id TEXT,
        comment_from_name TEXT,
        comment_from_id TEXT,
        comment_message TEXT,
        comment_created_time TEXT
    );
    """)

def testFacebookPageData(group_id, access_token):
    
    # construct the URL string
    base = "https://graph.facebook.com/v2.4"
    node = "/" + group_id
    parameters = "/?access_token=%s" % access_token
    url = base + node + parameters
    
    # retrieve data
    req = urllib2.Request(url)
    response = urllib2.urlopen(req)
    data = json.loads(response.read())
    
    print json.dumps(data, indent=4, sort_keys=True)
    

testFacebookPageData(group_id, access_token)

def request_until_succeed(url):
    req = urllib2.Request(url)
    success = False
    while success is False:
        try: 
            response = urllib2.urlopen(req)
            if response.getcode() == 200:
                success = True
        except Exception, e:
            print e
            time.sleep(5)
            
            print "Error for URL %s: %s" % (url, datetime.datetime.now())

    return response.read()



def testFacebookPageFeedData(group_id, access_token):
    
    # construct the URL string
    base = "https://graph.facebook.com/v2.4"
    node = "/" + group_id + "/feed" # changed
    parameters = "/?access_token=%s" % access_token
    url = base + node + parameters
    
    # retrieve data
    data = json.loads(request_until_succeed(url))
    print json.dumps(data, indent=4, sort_keys=True)
    

testFacebookPageFeedData(group_id, access_token)

def getFacebookGroupFeedData(group_id, access_token, num_group_posts):
    
    # construct the URL string
    base = "https://graph.facebook.com"
    node = "/" + group_id + "/feed" 
    parameters = "/?fields=from,story,place,message,link,created_time,updated_time,type,name,id,likes.limit(2000).summary(true),comments.limit(2000).summary(true){id,created_time,from,like_count,message,likes},shares&limit=%s&access_token=%s" % (num_group_posts, access_token) # changed
    url = base + node + parameters
    
    # retrieve data
    data = json.loads(request_until_succeed(url))
    
    return data
    

test_group_post = getFacebookGroupFeedData(group_id, access_token, 1)["data"][0]
print json.dumps(test_group_post, indent=4, sort_keys=True)

def processFacebookGroupFeedPost(group_post):
    
    # The group_post is now a Python dictionary, so for top-level items,
    # we can simply call the key.
    
    # Additionally, some items may not always exist,
    # so must check for existence first
    group_post_id = group_post['id'].encode('utf-8')
    group_post_from_id = '' if 'from' not in group_post.keys() else group_post['from']['id'].encode('utf-8')
    group_post_from_name = '' if 'from' not in group_post.keys() else group_post['from']['name'].encode('utf-8')
    group_post_message = '' if 'message' not in group_post.keys() else group_post['message'].encode('utf-8')
    group_post_story = '' if 'story' not in group_post.keys() else group_post['story'].encode('utf-8')
    group_post_placename = '' if 'place' not in group_post.keys() else group_post['place']['name'].encode('utf-8')
    link_name = '' if 'name' not in group_post.keys() else group_post['name'].encode('utf-8')
    group_post_type = group_post['type'].encode('utf-8')
    group_post_link = '' if 'link' not in group_post.keys() else group_post['link'].encode('utf-8')
    
    
    # Time needs special care since a) it's in UTC and
    # b) it's not easy to use in statistical programs.
    
    group_post_published = datetime.datetime.strptime(group_post['created_time'],'%Y-%m-%dT%H:%M:%S+0000')
    group_post_published = group_post_published + datetime.timedelta(hours=-5) # EST
    group_post_published = group_post_published.strftime('%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs

    group_post_updated = datetime.datetime.strptime(group_post['updated_time'],'%Y-%m-%dT%H:%M:%S+0000')
    group_post_updated = group_post_updated + datetime.timedelta(hours=-5) # EST
    group_post_updated = group_post_updated.strftime('%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs

    # Nested items require chaining dictionary keys.
    
    num_likes = 0 if 'likes' not in group_post.keys() else group_post['likes']['summary']['total_count']
    likes_uids = ""
    if 'likes' in group_post.keys():
        num_likes = group_post['likes']['summary']['total_count']
        likes_uids = ", ".join(pl['id'] for pl in group_post['likes']['data'])
    else:
        num_likes = 0
    comments_list = []
    if 'comments' in group_post.keys():
        num_comments = group_post['comments']['summary']['total_count']
        for comment in group_post['comments']['data']:
            comment_id = comment['id'].encode('utf-8')
            commenter = comment['from']['name'].encode('utf-8')
            commenter_id = comment['from']['id'].encode('utf-8')
            commenter_message = comment['message'].encode('utf-8')
            comment_likes = []
            if 'likes' in comment.keys():
                for cl in comment['likes']['data']:
                    comment_likes.append([comment_id, commenter, commenter_id, cl['name'].encode('utf-8'), cl['id']])
                cur.executemany('INSERT INTO likes VALUES (?,?,?,?,?)', comment_likes)
            commenter_time = datetime.datetime.strptime(comment['created_time'],'%Y-%m-%dT%H:%M:%S+0000')
            commenter_time = commenter_time + datetime.timedelta(hours=-5) # EST
            commenter_time = commenter_time.strftime('%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs
            comments_list.append([group_post_id, comment_id, commenter, commenter_id, commenter_message, commenter_time])
        cur.executemany('INSERT INTO comments VALUES (?,?,?,?,?,?)', comments_list)
    else:
        num_comments = 0
    num_shares = 0 if 'shares' not in group_post.keys() else group_post['shares']['count']
    cur.execute('INSERT INTO posts VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?)',
              (group_post_id, group_post_from_name, group_post_from_id, group_post_message, group_post_published,
              group_post_updated, group_post_story, group_post_placename, link_name, group_post_type, group_post_link,
              num_likes, num_comments, num_shares))

processed_test_group_post = processFacebookGroupFeedPost(test_group_post)

def scrapeFacebookGroupFeedPost(group_id, access_token):
    has_next_page = True
    num_processed = 0   # keep a count on how many we've processed
    scrape_starttime = datetime.datetime.now()
    
    print "Scraping %s Facebook Group: %s\n" % (group_id, scrape_starttime)
    
    group_posts = getFacebookGroupFeedData(group_id, access_token, 100)

    while has_next_page:
        for group_post in group_posts['data']:
            processed_group_post = processFacebookGroupFeedPost(group_post)
            # output progress occasionally to make sure code is not stalling
            num_processed += 1
            if num_processed % 1000 == 0:
                print "%s Statuses Processed: %s" % (num_processed, datetime.datetime.now())
                
        # if there is no next page, we're done.
        if 'paging' in group_posts.keys():
            group_posts = json.loads(request_until_succeed(group_posts['paging']['next']))
        else:
            has_next_page = False
            
    conn.commit()
    conn.close()
    print "\nDone!\n%s Statuses Processed in %s" % (num_processed, datetime.datetime.now() - scrape_starttime)

scrapeFacebookGroupFeedPost(group_id, access_token)

######### very simple query examples:
# most likes given:     select like_from_name, count(like_from_name) as count from likes group by like_from_name order by count desc limit 20;
# names with most comments: select comment_from_name, count(comment_from_name) as count from comments group by comment_from_name order by count desc limit 20;
# names with most posts:    select post_from_name, count(post_from_name) as count from posts group by post_from_name order by count desc limit 20;
